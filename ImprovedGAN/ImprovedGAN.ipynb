{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary of some of the more actionable tips is provided below.\n",
    "\n",
    "Normalize inputs to the range [-1, 1] and use tanh in the generator output.\n",
    "Flip the labels and loss function when training the generator.\n",
    "Sample Gaussian random numbers as input to the generator.\n",
    "Use mini batches of all real or all fake for calculating batch norm statistics.\n",
    "Use Leaky ReLU in the generator and discriminator.\n",
    "Use Average pooling and stride for downsampling; use ConvTranspose2D and stride for upsampling.\n",
    "Use label smoothing in the discriminator, with small random noise.\n",
    "Add random noise to the labels in the discriminator.\n",
    "Use DCGAN architecture, unless you have a good reason not to.\n",
    "A loss of 0.0 in the discriminator is a failure mode.\n",
    "If loss of the generator steadily decreases, it is likely fooling the discriminator with garbage images.\n",
    "Use labels if you have them.\n",
    "Add noise to inputs to the discriminator and decay the noise over time.\n",
    "Use dropout of 50 percent during train and generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following implementation is the semi-supervised learning.\n",
    "\n",
    "Labeled data --> [1,K] category\n",
    "\n",
    "Unlabeled data --> K+1 category\n",
    "\n",
    "Generated data --> K+1 category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Sleepychord/ImprovedGAN-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5,
     8,
     12
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import pdb\n",
    "def log_sum_exp(x, axis = 1):\n",
    "    m = torch.max(x, dim = 1)[0]\n",
    "    return m + torch.log(torch.sum(torch.exp(x - m.unsqueeze(1)), dim = axis))\n",
    "def reset_normal_param(L, stdv, weight_scale = 1.):\n",
    "    assert type(L) == torch.nn.Linear\n",
    "    torch.nn.init.normal(L.weight, std=weight_scale / math.sqrt(L.weight.size()[0]))\n",
    "    \n",
    "class LinearWeightNorm(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, weight_scale=None, weight_init_stdv=0.1):\n",
    "        super(LinearWeightNorm, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.randn(out_features, in_features) * weight_init_stdv)\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.zeros(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        if weight_scale is not None:\n",
    "            assert type(weight_scale) == int\n",
    "            self.weight_scale = Parameter(torch.ones(out_features, 1) * weight_scale)\n",
    "        else:\n",
    "            self.weight_scale = 1 \n",
    "    def forward(self, x):\n",
    "        W = self.weight * self.weight_scale / torch.sqrt(torch.sum(self.weight ** 2, dim = 1, keepdim = True))\n",
    "        return F.linear(x, W, self.bias)\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'in_features=' + str(self.in_features) \\\n",
    "            + ', out_features=' + str(self.out_features) \\\n",
    "            + ', weight_scale=' + str(self.weight_scale) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "import pdb\n",
    "from functional import reset_normal_param, LinearWeightNorm\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim = 28 ** 2, output_dim = 10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.layers = torch.nn.ModuleList([\n",
    "            LinearWeightNorm(input_dim, 1000),\n",
    "            LinearWeightNorm(1000, 500),\n",
    "            LinearWeightNorm(500, 250),\n",
    "            LinearWeightNorm(250, 250),\n",
    "            LinearWeightNorm(250, 250)]\n",
    "        )\n",
    "        self.final = LinearWeightNorm(250, output_dim, weight_scale=1)\n",
    "\n",
    "    def forward(self, x, feature = False, cuda = False):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        noise = torch.randn(x.size()) * 0.3 if self.training else torch.Tensor([0])\n",
    "        if cuda:\n",
    "            noise = noise.cuda()\n",
    "        x = x + Variable(noise, requires_grad = False)\n",
    "        for i in range(len(self.layers)):\n",
    "            m = self.layers[i]\n",
    "            x_f = F.relu(m(x))\n",
    "            noise = torch.randn(x_f.size()) * 0.5 if self.training else torch.Tensor([0])\n",
    "            if cuda:\n",
    "                noise = noise.cuda()\n",
    "            x = (x_f + Variable(noise, requires_grad = False))\n",
    "        if feature:\n",
    "            return x_f, self.final(x)\n",
    "        return self.final(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, output_dim = 28 ** 2):\n",
    "        super(Generator, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.fc1 = nn.Linear(z_dim, 500, bias = False)\n",
    "        self.bn1 = nn.BatchNorm1d(500, affine = False, eps=1e-6, momentum = 0.5)\n",
    "        self.fc2 = nn.Linear(500, 500, bias = False)\n",
    "        self.bn2 = nn.BatchNorm1d(500, affine = False, eps=1e-6, momentum = 0.5)\n",
    "        self.fc3 = LinearWeightNorm(500, output_dim, weight_scale = 1)\n",
    "        self.bn1_b = Parameter(torch.zeros(500))\n",
    "        self.bn2_b = Parameter(torch.zeros(500))\n",
    "        nn.init.xavier_uniform(self.fc1.weight)\n",
    "        nn.init.xavier_uniform(self.fc2.weight)\n",
    "\n",
    "    def forward(self, batch_size, cuda = False):\n",
    "        x = Variable(torch.rand(batch_size, self.z_dim), requires_grad = False, volatile = not self.training)\n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "        x = F.softplus(self.bn1(self.fc1(x)) + self.bn1_b)\n",
    "        x = F.softplus(self.bn2(self.fc2(x)) + self.bn2_b)\n",
    "        x = F.softplus(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     18,
     19,
     23,
     30,
     41
    ]
   },
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "from __future__ import print_function \n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from functional import log_sum_exp\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import sys\n",
    "import argparse\n",
    "from Nets import Generator, Discriminator\n",
    "from Datasets import *\n",
    "import pdb\n",
    "import tensorboardX\n",
    "import os\n",
    "class ImprovedGAN(object):\n",
    "    def __init__(self, G, D, labeled, unlabeled, test, args):\n",
    "        if os.path.exists(args.savedir):\n",
    "            print('Loading model from ' + args.savedir)\n",
    "            self.G = torch.load(os.path.join(args.savedir, 'G.pkl'))\n",
    "            self.D = torch.load(os.path.join(args.savedir, 'D.pkl'))\n",
    "        else:\n",
    "            os.makedirs(args.savedir)\n",
    "            self.G = G\n",
    "            self.D = D\n",
    "            torch.save(self.G, os.path.join(args.savedir, 'G.pkl'))\n",
    "            torch.save(self.D, os.path.join(args.savedir, 'D.pkl'))\n",
    "        self.writer = tensorboardX.SummaryWriter(log_dir=args.logdir)\n",
    "        if args.cuda:\n",
    "            self.G.cuda()\n",
    "            self.D.cuda()\n",
    "        self.labeled = labeled\n",
    "        self.unlabeled = unlabeled\n",
    "        self.test = test\n",
    "        self.Doptim = optim.Adam(self.D.parameters(), lr=args.lr, betas= (args.momentum, 0.999))\n",
    "        self.Goptim = optim.Adam(self.G.parameters(), lr=args.lr, betas = (args.momentum,0.999))\n",
    "        self.args = args\n",
    "    def trainD(self, x_label, y, x_unlabel):\n",
    "        x_label, x_unlabel, y = Variable(x_label), Variable(x_unlabel), Variable(y, requires_grad = False)\n",
    "        if self.args.cuda:\n",
    "            x_label, x_unlabel, y = x_label.cuda(), x_unlabel.cuda(), y.cuda()\n",
    "        output_label = self.D(x_label, cuda=self.args.cuda) \n",
    "        output_unlabel = self.D(x_unlabel, cuda=self.args.cuda)\n",
    "        output_fake = self.D(self.G(x_unlabel.size()[0], cuda = self.args.cuda).view(x_unlabel.size()).detach(), cuda=self.args.cuda)\n",
    "        logz_label, logz_unlabel, logz_fake = log_sum_exp(output_label), log_sum_exp(output_unlabel), log_sum_exp(output_fake) # log âˆ‘e^x_i\n",
    "        prob_label = torch.gather(output_label, 1, y.unsqueeze(1)) # log e^x_label = x_label \n",
    "        loss_supervised = -torch.mean(prob_label) + torch.mean(logz_label)\n",
    "        loss_unsupervised = 0.5 * (-torch.mean(logz_unlabel) + torch.mean(F.softplus(logz_unlabel))  + # real_data: log Z/(1+Z)\n",
    "                            torch.mean(F.softplus(logz_fake)) ) # fake_data: log 1/(1+Z)\n",
    "        loss = loss_supervised + self.args.unlabel_weight * loss_unsupervised\n",
    "        acc = torch.mean((output_label.max(1)[1] == y).float())\n",
    "        self.Doptim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.Doptim.step()\n",
    "        return loss_supervised.data.cpu().numpy(), loss_unsupervised.data.cpu().numpy(), acc\n",
    "    \n",
    "    def trainG(self, x_unlabel):\n",
    "        fake = self.G(x_unlabel.size()[0], cuda = self.args.cuda).view(x_unlabel.size())\n",
    "        mom_gen, output_fake = self.D(fake, feature=True, cuda=self.args.cuda)\n",
    "        mom_unlabel, _ = self.D(Variable(x_unlabel), feature=True, cuda=self.args.cuda)\n",
    "        mom_gen = torch.mean(mom_gen, dim = 0)\n",
    "        mom_unlabel = torch.mean(mom_unlabel, dim = 0)\n",
    "        loss_fm = torch.mean((mom_gen - mom_unlabel) ** 2)\n",
    "        loss = loss_fm \n",
    "        self.Goptim.zero_grad()\n",
    "        self.Doptim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.Goptim.step()\n",
    "        return loss.data.cpu().numpy()\n",
    "\n",
    "    def train(self):\n",
    "        assert self.unlabeled.__len__() > self.labeled.__len__()\n",
    "        assert type(self.labeled) == TensorDataset\n",
    "        times = int(np.ceil(self.unlabeled.__len__() * 1. / self.labeled.__len__()))\n",
    "        t1 = self.labeled.tensors[0].clone()\n",
    "        t2 = self.labeled.tensors[1].clone()\n",
    "        tile_labeled = TensorDataset(t1.repeat(times,1,1,1),t2.repeat(times))\n",
    "        gn = 0\n",
    "        for epoch in range(self.args.epochs):\n",
    "            self.G.train()\n",
    "            self.D.train()\n",
    "            unlabel_loader1 = DataLoader(self.unlabeled, batch_size = self.args.batch_size, shuffle=True, drop_last=True, num_workers = 4)\n",
    "            unlabel_loader2 = DataLoader(self.unlabeled, batch_size = self.args.batch_size, shuffle=True, drop_last=True, num_workers = 4).__iter__()\n",
    "            label_loader = DataLoader(tile_labeled, batch_size = self.args.batch_size, shuffle=True, drop_last=True, num_workers = 4).__iter__()\n",
    "            loss_supervised = loss_unsupervised = loss_gen = accuracy = 0.\n",
    "            batch_num = 0\n",
    "            for (unlabel1, _label1) in unlabel_loader1:\n",
    "                batch_num += 1\n",
    "                unlabel2, _label2 = unlabel_loader2.next()\n",
    "                x, y = label_loader.next()\n",
    "                if args.cuda:\n",
    "                    x, y, unlabel1, unlabel2 = x.cuda(), y.cuda(), unlabel1.cuda(), unlabel2.cuda()\n",
    "                ll, lu, acc = self.trainD(x, y, unlabel1)\n",
    "                loss_supervised += ll\n",
    "                loss_unsupervised += lu\n",
    "                accuracy += acc\n",
    "                lg = self.trainG(unlabel2)\n",
    "                if epoch > 1 and lg > 1:\n",
    "                    lg = self.trainG(unlabel2)\n",
    "                loss_gen += lg\n",
    "                if (batch_num + 1) % self.args.log_interval == 0:\n",
    "                    print('Training: %d / %d' % (batch_num + 1, len(unlabel_loader1)))\n",
    "                    gn += 1\n",
    "                    with torch.no_grad():\n",
    "                        self.writer.add_scalars('loss', {'loss_supervised':ll, 'loss_unsupervised':lu, 'loss_gen':lg}, gn)\n",
    "                        self.writer.add_histogram('real_feature', self.D(Variable(x), cuda=self.args.cuda, feature = True)[0], gn)\n",
    "                        self.writer.add_histogram('fake_feature', self.D(self.G(self.args.batch_size, cuda = self.args.cuda), cuda=self.args.cuda, feature = True)[0], gn)\n",
    "                        self.writer.add_histogram('fc3_bias', self.G.fc3.bias, gn)\n",
    "                        self.writer.add_histogram('D_feature_weight', self.D.layers[-1].weight, gn)\n",
    "                    self.D.train()\n",
    "                    self.G.train()\n",
    "            loss_supervised /= batch_num\n",
    "            loss_unsupervised /= batch_num\n",
    "            loss_gen /= batch_num\n",
    "            accuracy /= batch_num\n",
    "            print(\"Iteration %d, loss_supervised = %.4f, loss_unsupervised = %.4f, loss_gen = %.4f train acc = %.4f\" % (epoch, loss_supervised, loss_unsupervised, loss_gen, accuracy))\n",
    "            sys.stdout.flush()\n",
    "            if (epoch + 1) % self.args.eval_interval == 0:\n",
    "                print(\"Eval: correct %d / %d\"  % (self.eval(), self.test.__len__()))\n",
    "                torch.save(self.G, os.path.join(args.savedir, 'G.pkl'))\n",
    "                torch.save(self.D, os.path.join(args.savedir, 'D.pkl'))\n",
    "                \n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            ret = torch.max(self.D(Variable(x), cuda=self.args.cuda), 1)[1].data\n",
    "        return ret\n",
    "\n",
    "    def eval(self):\n",
    "        self.G.eval()\n",
    "        self.D.eval()\n",
    "        d, l = [], []\n",
    "        for (datum, label) in self.test:\n",
    "            d.append(datum)\n",
    "            l.append(label)\n",
    "        x, y = torch.stack(d), torch.LongTensor(l)\n",
    "        if self.args.cuda:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "        pred = self.predict(x)\n",
    "        return torch.sum(pred == y)\n",
    "    def draw(self, batch_size):\n",
    "        self.G.eval()\n",
    "        return self.G(batch_size, cuda=self.args.cuda)\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Improved GAN')\n",
    "    parser.add_argument('--batch-size', type=int, default=100, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                        help='number of epochs to train (default: 10)')\n",
    "    parser.add_argument('--lr', type=float, default=0.003, metavar='LR',\n",
    "                        help='learning rate (default: 0.003)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                        help='SGD momentum (default: 0.5)')\n",
    "    parser.add_argument('--cuda', action='store_true', default=False,\n",
    "                        help='CUDA training')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--eval-interval', type=int, default=1, metavar='N',\n",
    "                        help='how many epochs to wait before evaling training status')\n",
    "    parser.add_argument('--unlabel-weight', type=float, default=1, metavar='N',\n",
    "                        help='scale factor between labeled and unlabeled data')\n",
    "    parser.add_argument('--logdir', type=str, default='./logfile', metavar='LOG_PATH', help='logfile path, tensorboard format')\n",
    "    parser.add_argument('--savedir', type=str, default='./models', metavar='SAVE_PATH', help = 'saving path, pickle format')\n",
    "    args = parser.parse_args()\n",
    "    args.cuda = args.cuda and torch.cuda.is_available()\n",
    "    np.random.seed(args.seed)\n",
    "    gan = ImprovedGAN(Generator(100), Discriminator(), MnistLabel(10), MnistUnlabel(), MnistTest(), args)\n",
    "    gan.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
