{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use latent representation to solve auxiliary problems.\n",
    "\n",
    "Projecting data back into the latent space.\n",
    "\n",
    "The discriminator D discriminates not only in data space (x versus G(z)) but jointly in data and latent space ((x,E(x)) versus (G(z),z)).\n",
    "\n",
    "$$\\min_{G,E}\\max_{D} V(D,E,G)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V(D,E,G)=\\mathbb{E}_{x\\sim p_x}[\\mathbb{E}_{z\\sim p_E(.|x)}\\log D(x,z)]+\\mathbb{E}_{z\\sim p_z}\\left[\\mathbb{E}_{x\\sim p_G(.|z)}\\log(1-D(x,z))\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In implementation: \n",
    "\n",
    "d_real--E-->z_real (sigma and mu)--> real_noise~N(mu,sigma)\n",
    "fake_noise--G-->d_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5,
     7,
     46,
     97,
     105,
     153
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        self.output_bias = nn.Parameter(torch.zeros(3, 32, 32), requires_grad=True)\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.latent_size, 256, 4, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(32, 32, 5, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(32, 32, 1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(32, 3, 1, stride=1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        output = F.sigmoid(output + self.output_bias)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_size, noise=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        if noise:\n",
    "            self.latent_size *= 2\n",
    "        self.main1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 5, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(32, 64, 4, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.main2 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 4, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.main3 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.main4 = nn.Sequential(\n",
    "            nn.Conv2d(512, self.latent_size, 1, stride=1, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size = input.size()[0]\n",
    "        x1 = self.main1(input)\n",
    "        x2 = self.main2(x1)\n",
    "        x3 = self.main3(x2)\n",
    "        output = self.main4(x3)\n",
    "        return output, x3.view(batch_size, -1), x2.view(batch_size, -1), x1.view(batch_size, -1)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_size, dropout, output_size=10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.dropout = dropout\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.infer_x = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 5, stride=1, bias=True),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout2d(p=self.dropout),\n",
    "\n",
    "            nn.Conv2d(32, 64, 4, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout2d(p=self.dropout),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout2d(p=self.dropout),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout2d(p=self.dropout),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout2d(p=self.dropout)\n",
    "        )\n",
    "\n",
    "        self.infer_z = nn.Sequential(\n",
    "            nn.Conv2d(self.latent_size, 512, 1, stride=1, bias=False),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout2d(p=self.dropout),\n",
    "\n",
    "            nn.Conv2d(512, 512, 1, stride=1, bias=False),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout2d(p=self.dropout)\n",
    "        )\n",
    "\n",
    "        self.infer_joint = nn.Sequential(\n",
    "            nn.Conv2d(1024, 1024, 1, stride=1, bias=True),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout2d(p=self.dropout),\n",
    "\n",
    "            nn.Conv2d(1024, 1024, 1, stride=1, bias=True),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout2d(p=self.dropout)\n",
    "        )\n",
    "\n",
    "        self.final = nn.Conv2d(1024, self.output_size, 1, stride=1, bias=True)\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        output_x = self.infer_x(x)\n",
    "        output_z = self.infer_z(z)\n",
    "        output_features = self.infer_joint(torch.cat([output_x, output_z], dim=1))\n",
    "        output = self.final(output_features)\n",
    "        if self.output_size == 1:\n",
    "            output = F.sigmoid(output)\n",
    "        return output.squeeze(), output_features.view(x.size()[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as vutils\n",
    "from model import *\n",
    "import os\n",
    "\n",
    "batch_size = 100\n",
    "lr = 1e-4\n",
    "latent_size = 256\n",
    "num_epochs = 100\n",
    "cuda_device = \"0\"\n",
    "\n",
    "\n",
    "def boolean_string(s):\n",
    "    if s not in {'False', 'True'}:\n",
    "        raise ValueError('Not a valid boolean string')\n",
    "    return s == 'True'\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', required=True, help='cifar10 | svhn')\n",
    "parser.add_argument('--dataroot', required=True, help='path to dataset')\n",
    "parser.add_argument('--use_cuda', type=boolean_string, default=True)\n",
    "parser.add_argument('--save_model_dir', required=True)\n",
    "parser.add_argument('--save_image_dir', required=True)\n",
    "\n",
    "opt = parser.parse_args()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_device\n",
    "print(opt)\n",
    "\n",
    "def tocuda(x):\n",
    "    if opt.use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "def log_sum_exp(input):\n",
    "    m, _ = torch.max(input, dim=1, keepdim=True)\n",
    "    input0 = input - m\n",
    "    m.squeeze()\n",
    "    return m + torch.log(torch.sum(torch.exp(input0), dim=1))\n",
    "\n",
    "\n",
    "def get_log_odds(raw_marginals):\n",
    "    marginals = torch.clamp(raw_marginals.mean(dim=0), 1e-7, 1 - 1e-7)\n",
    "    return torch.log(marginals / (1 - marginals))\n",
    "\n",
    "\n",
    "if opt.dataset == 'svhn':\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.SVHN(root=opt.dataroot, split='extra', download=True,\n",
    "                      transform=transforms.Compose([\n",
    "                          transforms.ToTensor()\n",
    "                      ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "elif opt.dataset == 'cifar10':\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(root=opt.dataroot, train=True, download=True,\n",
    "                      transform=transforms.Compose([\n",
    "                          transforms.ToTensor()\n",
    "                      ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "netE = tocuda(Encoder(latent_size, True))\n",
    "netG = tocuda(Generator(latent_size))\n",
    "netD = tocuda(Discriminator(latent_size, 0.2, 1))\n",
    "\n",
    "netE.apply(weights_init)\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "optimizerG = optim.Adam([{'params' : netE.parameters()},\n",
    "                         {'params' : netG.parameters()}], lr=lr, betas=(0.5,0.999))\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    i = 0\n",
    "    for (data, target) in train_loader:\n",
    "\n",
    "        real_label = Variable(tocuda(torch.ones(batch_size)))\n",
    "        fake_label = Variable(tocuda(torch.zeros(batch_size)))\n",
    "\n",
    "        noise1 = Variable(tocuda(torch.Tensor(data.size()).normal_(0, 0.1 * (num_epochs - epoch) / num_epochs)))\n",
    "        noise2 = Variable(tocuda(torch.Tensor(data.size()).normal_(0, 0.1 * (num_epochs - epoch) / num_epochs)))\n",
    "\n",
    "        if epoch == 0 and i == 0:\n",
    "            netG.output_bias.data = get_log_odds(tocuda(data))\n",
    "\n",
    "        if data.size()[0] != batch_size:\n",
    "            continue\n",
    "\n",
    "        d_real = Variable(tocuda(data))\n",
    "\n",
    "        z_fake = Variable(tocuda(torch.randn(batch_size, latent_size, 1, 1)))\n",
    "        d_fake = netG(z_fake)\n",
    "\n",
    "        z_real, _, _, _ = netE(d_real)\n",
    "        z_real = z_real.view(batch_size, -1)\n",
    "\n",
    "        mu, log_sigma = z_real[:, :latent_size], z_real[:, latent_size:]\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        epsilon = Variable(tocuda(torch.randn(batch_size, latent_size)))\n",
    "\n",
    "        output_z = mu + epsilon * sigma\n",
    "\n",
    "        output_real, _ = netD(d_real + noise1, output_z.view(batch_size, latent_size, 1, 1))\n",
    "        output_fake, _ = netD(d_fake + noise2, z_fake)\n",
    "\n",
    "        loss_d = criterion(output_real, real_label) + criterion(output_fake, fake_label)\n",
    "        loss_g = criterion(output_fake, real_label) + criterion(output_real, fake_label)\n",
    "\n",
    "        if loss_g.data[0] < 3.5:\n",
    "            optimizerD.zero_grad()\n",
    "            loss_d.backward(retain_graph=True)\n",
    "            optimizerD.step()\n",
    "\n",
    "        optimizerG.zero_grad()\n",
    "        loss_g.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i % 1 == 0:\n",
    "            print(\"Epoch :\", epoch, \"Iter :\", i, \"D Loss :\", loss_d.data[0], \"G loss :\", loss_g.data[0],\n",
    "                  \"D(x) :\", output_real.mean().data[0], \"D(G(x)) :\", output_fake.mean().data[0])\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            vutils.save_image(d_fake.cpu().data[:16, ], './%s/fake.png' % (opt.save_image_dir))\n",
    "            vutils.save_image(d_real.cpu().data[:16, ], './%s/real.png'% (opt.save_image_dir))\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(netG.state_dict(), './%s/netG_epoch_%d.pth' % (opt.save_model_dir, epoch))\n",
    "        torch.save(netE.state_dict(), './%s/netE_epoch_%d.pth' % (opt.save_model_dir, epoch))\n",
    "        torch.save(netD.state_dict(), './%s/netD_epoch_%d.pth' % (opt.save_model_dir, epoch))\n",
    "\n",
    "        vutils.save_image(d_fake.cpu().data[:16, ], './%s/fake_%d.png' % (opt.save_image_dir, epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
