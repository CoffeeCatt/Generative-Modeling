{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-valued non-volume preserving (real NVP) transformation.\n",
    "\n",
    "Change of variable formula:\n",
    "\n",
    "$x--f-->z$, \n",
    "    $$p_X(x) = p_Z(f(x))\\left|\\det \\left(\\frac{\\partial f(x)}{\\partial x^\\top}\\right)\\right|$$\n",
    "    $$\\log p_X(x) = \\log(p_Z(f(x))) + \\log \\left(\\left|\\det \\left(\\frac{\\partial f(x)}{\\partial x^\\top}\\right)\\right|\\right)$$\n",
    "    \n",
    "determinants of large matrices are in general computationally very expensive --> carefully design the function f.\n",
    "\n",
    "Affine coupling layer\n",
    "$$y_{1:d} = x_{1:d}$$\n",
    "$$y_{d+1:D} = x_{d+1:D}\\circ \\exp(s(x_{1:d})) + t(x_{1:d})$$\n",
    "\n",
    "\n",
    "Implementation:\n",
    "data $x\\sim p(x)$\n",
    "forward:x->z or $x->u$\n",
    "\n",
    "inverse:z->x or u->x\n",
    "$$y_{1:d} = \\text{mask}*x$$\n",
    "$$y_{d+1:D} = (1-\\text{mask})*\\exp(s(\\text{mask}*x))+t(\\text{mask}*x)$$\n",
    "--> $$u =\\text{mask}*x+(1-\\text{mask})*\\exp(s(\\text{mask}*x))+t(\\text{mask}*x) $$\n",
    "log determinant:\n",
    "$$(1 - mask) * s(\\text{mask}*x)$$\n",
    "\n",
    "Combining coupling layers:\n",
    "alternating masks.\n",
    "\n",
    "log_prob:\n",
    "max log p(x) = max (log p(u) + log det J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/kamenbliznashki/normalizing_flows/blob/master/maf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     64,
     71,
     79,
     109,
     111,
     123,
     147,
     241,
     281,
     312,
     345,
     367,
     376,
     426
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Masked Autoregressive Flow for Density Estimation\n",
    "arXiv:1705.07057v4\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import math\n",
    "import argparse\n",
    "import pprint\n",
    "import copy\n",
    "\n",
    "from data import fetch_dataloaders\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# action\n",
    "parser.add_argument('--train', action='store_true', help='Train a flow.')\n",
    "parser.add_argument('--evaluate', action='store_true', help='Evaluate a flow.')\n",
    "parser.add_argument('--restore_file', type=str, help='Path to model to restore.')\n",
    "parser.add_argument('--generate', action='store_true', help='Generate samples from a model.')\n",
    "parser.add_argument('--data_dir', default='./data/', help='Location of datasets.')\n",
    "parser.add_argument('--output_dir', default='./results/{}'.format(os.path.splitext(__file__)[0]))\n",
    "parser.add_argument('--results_file', default='results.txt', help='Filename where to store settings and test results.')\n",
    "parser.add_argument('--no_cuda', action='store_true', help='Do not use cuda.')\n",
    "# data\n",
    "parser.add_argument('--dataset', default='toy', help='Which dataset to use.')\n",
    "parser.add_argument('--flip_toy_var_order', action='store_true', help='Whether to flip the toy dataset variable order to (x2, x1).')\n",
    "parser.add_argument('--seed', type=int, default=1, help='Random seed to use.')\n",
    "# model\n",
    "parser.add_argument('--model', default='maf', help='Which model to use: made, maf.')\n",
    "# made parameters\n",
    "parser.add_argument('--n_blocks', type=int, default=5, help='Number of blocks to stack in a model (MADE in MAF; Coupling+BN in RealNVP).')\n",
    "parser.add_argument('--n_components', type=int, default=1, help='Number of Gaussian clusters for mixture of gaussians models.')\n",
    "parser.add_argument('--hidden_size', type=int, default=100, help='Hidden layer size for MADE (and each MADE block in an MAF).')\n",
    "parser.add_argument('--n_hidden', type=int, default=1, help='Number of hidden layers in each MADE.')\n",
    "parser.add_argument('--activation_fn', type=str, default='relu', help='What activation function to use in the MADEs.')\n",
    "parser.add_argument('--input_order', type=str, default='sequential', help='What input order to use (sequential | random).')\n",
    "parser.add_argument('--conditional', default=False, action='store_true', help='Whether to use a conditional model.')\n",
    "parser.add_argument('--no_batch_norm', action='store_true')\n",
    "# training params\n",
    "parser.add_argument('--batch_size', type=int, default=100)\n",
    "parser.add_argument('--n_epochs', type=int, default=50)\n",
    "parser.add_argument('--start_epoch', default=0, help='Starting epoch (for logging; to be overwritten when restoring file.')\n",
    "parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate.')\n",
    "parser.add_argument('--log_interval', type=int, default=1000, help='How often to show loss statistics and save samples.')\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Model layers and helpers\n",
    "# --------------------\n",
    "\n",
    "class LinearMaskedCoupling(nn.Module):\n",
    "    \"\"\" Modified RealNVP Coupling Layers per the MAF paper \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, n_hidden, mask, cond_label_size=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "        # scale function\n",
    "        s_net = [nn.Linear(input_size + (cond_label_size if cond_label_size is not None else 0), hidden_size)]\n",
    "        for _ in range(n_hidden):\n",
    "            s_net += [nn.Tanh(), nn.Linear(hidden_size, hidden_size)]\n",
    "        s_net += [nn.Tanh(), nn.Linear(hidden_size, input_size)]\n",
    "        self.s_net = nn.Sequential(*s_net)\n",
    "\n",
    "        # translation function\n",
    "        self.t_net = copy.deepcopy(self.s_net)\n",
    "        # replace Tanh with ReLU's per MAF paper\n",
    "        for i in range(len(self.t_net)):\n",
    "            if not isinstance(self.t_net[i], nn.Linear): self.t_net[i] = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        # apply mask\n",
    "        mx = x * self.mask\n",
    "\n",
    "        # run through model\n",
    "        s = self.s_net(mx if y is None else torch.cat([y, mx], dim=1))\n",
    "        t = self.t_net(mx if y is None else torch.cat([y, mx], dim=1))\n",
    "        u = mx + (1 - self.mask) * (x - t) * torch.exp(-s)  # cf RealNVP eq 8 where u corresponds to x (here we're modeling u)\n",
    "\n",
    "        log_abs_det_jacobian = - (1 - self.mask) * s  # log det du/dx; cf RealNVP 8 and 6; note, sum over input_size done at model log_prob\n",
    "\n",
    "        return u, log_abs_det_jacobian\n",
    "\n",
    "    def inverse(self, u, y=None):\n",
    "        # apply mask\n",
    "        mu = u * self.mask\n",
    "\n",
    "        # run through model\n",
    "        s = self.s_net(mu if y is None else torch.cat([y, mu], dim=1))\n",
    "        t = self.t_net(mu if y is None else torch.cat([y, mu], dim=1))\n",
    "        x = mu + (1 - self.mask) * (u * s.exp() + t)  # cf RealNVP eq 7\n",
    "\n",
    "        log_abs_det_jacobian = (1 - self.mask) * s  # log det dx/du\n",
    "\n",
    "        return x, log_abs_det_jacobian\n",
    "\n",
    "\n",
    "class BatchNorm(nn.Module):\n",
    "    \"\"\" RealNVP BatchNorm layer \"\"\"\n",
    "    def __init__(self, input_size, momentum=0.9, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "        self.log_gamma = nn.Parameter(torch.zeros(input_size))\n",
    "        self.beta = nn.Parameter(torch.zeros(input_size))\n",
    "\n",
    "        self.register_buffer('running_mean', torch.zeros(input_size))\n",
    "        self.register_buffer('running_var', torch.ones(input_size))\n",
    "\n",
    "    def forward(self, x, cond_y=None):\n",
    "        if self.training:\n",
    "            self.batch_mean = x.mean(0)\n",
    "            self.batch_var = x.var(0) # note MAF paper uses biased variance estimate; ie x.var(0, unbiased=False)\n",
    "\n",
    "            # update running mean\n",
    "            self.running_mean.mul_(self.momentum).add_(self.batch_mean.data * (1 - self.momentum))\n",
    "            self.running_var.mul_(self.momentum).add_(self.batch_var.data * (1 - self.momentum))\n",
    "\n",
    "            mean = self.batch_mean\n",
    "            var = self.batch_var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "\n",
    "        # compute normalized input (cf original batch norm paper algo 1)\n",
    "        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        y = self.log_gamma.exp() * x_hat + self.beta\n",
    "\n",
    "        # compute log_abs_det_jacobian (cf RealNVP paper)\n",
    "        log_abs_det_jacobian = self.log_gamma - 0.5 * torch.log(var + self.eps)\n",
    "#        print('in sum log var {:6.3f} ; out sum log var {:6.3f}; sum log det {:8.3f}; mean log_gamma {:5.3f}; mean beta {:5.3f}'.format(\n",
    "#            (var + self.eps).log().sum().data.numpy(), y.var(0).log().sum().data.numpy(), log_abs_det_jacobian.mean(0).item(), self.log_gamma.mean(), self.beta.mean()))\n",
    "        return y, log_abs_det_jacobian.expand_as(x)\n",
    "\n",
    "    def inverse(self, y, cond_y=None):\n",
    "        if self.training:\n",
    "            mean = self.batch_mean\n",
    "            var = self.batch_var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "\n",
    "        x_hat = (y - self.beta) * torch.exp(-self.log_gamma)\n",
    "        x = x_hat * torch.sqrt(var + self.eps) + mean\n",
    "\n",
    "        log_abs_det_jacobian = 0.5 * torch.log(var + self.eps) - self.log_gamma\n",
    "\n",
    "        return x, log_abs_det_jacobian.expand_as(x)\n",
    "\n",
    "\n",
    "class FlowSequential(nn.Sequential):\n",
    "    \"\"\" Container for layers of a normalizing flow \"\"\"\n",
    "    def forward(self, x, y):\n",
    "        sum_log_abs_det_jacobians = 0\n",
    "        for module in self:\n",
    "            x, log_abs_det_jacobian = module(x, y)\n",
    "            sum_log_abs_det_jacobians = sum_log_abs_det_jacobians + log_abs_det_jacobian\n",
    "        return x, sum_log_abs_det_jacobians\n",
    "\n",
    "    def inverse(self, u, y):\n",
    "        sum_log_abs_det_jacobians = 0\n",
    "        for module in reversed(self):\n",
    "            u, log_abs_det_jacobian = module.inverse(u, y)\n",
    "            sum_log_abs_det_jacobians = sum_log_abs_det_jacobians + log_abs_det_jacobian\n",
    "        return u, sum_log_abs_det_jacobians\n",
    "\n",
    "\n",
    "class RealNVP(nn.Module):\n",
    "    def __init__(self, n_blocks, input_size, hidden_size, n_hidden, cond_label_size=None, batch_norm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # base distribution for calculation of log prob under the model\n",
    "        self.register_buffer('base_dist_mean', torch.zeros(input_size))\n",
    "        self.register_buffer('base_dist_var', torch.ones(input_size))\n",
    "\n",
    "        # construct model\n",
    "        modules = []\n",
    "        mask = torch.arange(input_size).float() % 2\n",
    "        for i in range(n_blocks):\n",
    "            modules += [LinearMaskedCoupling(input_size, hidden_size, n_hidden, mask, cond_label_size)]\n",
    "            mask = 1 - mask\n",
    "            modules += batch_norm * [BatchNorm(input_size)]\n",
    "\n",
    "        self.net = FlowSequential(*modules)\n",
    "\n",
    "    @property\n",
    "    def base_dist(self):\n",
    "        return D.Normal(self.base_dist_mean, self.base_dist_var)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        return self.net(x, y)\n",
    "\n",
    "    def inverse(self, u, y=None):\n",
    "        return self.net.inverse(u, y)\n",
    "\n",
    "    def log_prob(self, x, y=None):\n",
    "        u, sum_log_abs_det_jacobians = self.forward(x, y)\n",
    "        return torch.sum(self.base_dist.log_prob(u) + sum_log_abs_det_jacobians, dim=1)\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Train and evaluate\n",
    "# --------------------\n",
    "\n",
    "def train(model, dataloader, optimizer, epoch, args):\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "        model.train()\n",
    "\n",
    "        # check if labeled dataset\n",
    "        if len(data) == 1:\n",
    "            x, y = data[0], None\n",
    "        else:\n",
    "            x, y = data\n",
    "            y = y.to(args.device)\n",
    "        x = x.view(x.shape[0], -1).to(args.device)\n",
    "\n",
    "        loss = - model.log_prob(x, y if args.cond_label_size else None).mean(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % args.log_interval == 0:\n",
    "            print('epoch {:3d} / {}, step {:4d} / {}; loss {:.4f}'.format(\n",
    "                epoch, args.start_epoch + args.n_epochs, i, len(dataloader), loss.item()))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, epoch, args):\n",
    "    model.eval()\n",
    "\n",
    "    # conditional model\n",
    "    if args.cond_label_size is not None:\n",
    "        logprior = torch.tensor(1 / args.cond_label_size).log().to(args.device)\n",
    "        loglike = [[] for _ in range(args.cond_label_size)]\n",
    "\n",
    "        for i in range(args.cond_label_size):\n",
    "            # make one-hot labels\n",
    "            labels = torch.zeros(args.batch_size, args.cond_label_size).to(args.device)\n",
    "            labels[:,i] = 1\n",
    "\n",
    "            for x, y in dataloader:\n",
    "                x = x.view(x.shape[0], -1).to(args.device)\n",
    "                loglike[i].append(model.log_prob(x, labels))\n",
    "\n",
    "            loglike[i] = torch.cat(loglike[i], dim=0)   # cat along data dim under this label\n",
    "        loglike = torch.stack(loglike, dim=1)           # cat all data along label dim\n",
    "\n",
    "        # log p(x) = log ∑_y p(x,y) = log ∑_y p(x|y)p(y)\n",
    "        # assume uniform prior      = log p(y) ∑_y p(x|y) = log p(y) + log ∑_y p(x|y)\n",
    "        logprobs = logprior + loglike.logsumexp(dim=1)\n",
    "        # TODO -- measure accuracy as argmax of the loglike\n",
    "\n",
    "    # unconditional model\n",
    "    else:\n",
    "        logprobs = []\n",
    "        for data in dataloader:\n",
    "            x = data[0].view(data[0].shape[0], -1).to(args.device)\n",
    "            logprobs.append(model.log_prob(x))\n",
    "        logprobs = torch.cat(logprobs, dim=0).to(args.device)\n",
    "\n",
    "    logprob_mean, logprob_std = logprobs.mean(0), 2 * logprobs.var(0).sqrt() / math.sqrt(len(dataloader.dataset))\n",
    "    output = 'Evaluate ' + (epoch != None)*'(epoch {}) -- '.format(epoch) + 'logp(x) = {:.3f} +/- {:.3f}'.format(logprob_mean, logprob_std)\n",
    "    print(output)\n",
    "    print(output, file=open(args.results_file, 'a'))\n",
    "    return logprob_mean, logprob_std\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, dataset_lam, args, step=None, n_row=10):\n",
    "    model.eval()\n",
    "\n",
    "    # conditional model\n",
    "    if args.cond_label_size:\n",
    "        samples = []\n",
    "        labels = torch.eye(args.cond_label_size).to(args.device)\n",
    "\n",
    "        for i in range(args.cond_label_size):\n",
    "            # sample model base distribution and run through inverse model to sample data space\n",
    "            u = model.base_dist.sample((n_row, args.n_components)).squeeze()\n",
    "            labels_i = labels[i].expand(n_row, -1)\n",
    "            sample, _ = model.inverse(u, labels_i)\n",
    "            log_probs = model.log_prob(sample, labels_i).sort(0)[1].flip(0)  # sort by log_prob; take argsort idxs; flip high to low\n",
    "            samples.append(sample[log_probs])\n",
    "\n",
    "        samples = torch.cat(samples, dim=0)\n",
    "\n",
    "    # unconditional model\n",
    "    else:\n",
    "        u = model.base_dist.sample((n_row**2, args.n_components)).squeeze()\n",
    "        samples, _ = model.inverse(u)\n",
    "        log_probs = model.log_prob(samples).sort(0)[1].flip(0)  # sort by log_prob; take argsort idxs; flip high to low\n",
    "        samples = samples[log_probs]\n",
    "\n",
    "    # convert and save images\n",
    "    samples = samples.view(samples.shape[0], *args.input_dims)\n",
    "    samples = (torch.sigmoid(samples) - dataset_lam) / (1 - 2 * dataset_lam)\n",
    "    filename = 'generated_samples' + (step != None)*'_epoch_{}'.format(step) + '.png'\n",
    "    save_image(samples, os.path.join(args.output_dir, filename), nrow=n_row, normalize=True)\n",
    "\n",
    "def train_and_evaluate(model, train_loader, test_loader, optimizer, args):\n",
    "    best_eval_logprob = float('-inf')\n",
    "\n",
    "    for i in range(args.start_epoch, args.start_epoch + args.n_epochs):\n",
    "        train(model, train_loader, optimizer, i, args)\n",
    "        eval_logprob, _ = evaluate(model, test_loader, i, args)\n",
    "\n",
    "        # save training checkpoint\n",
    "        torch.save({'epoch': i,\n",
    "                    'model_state': model.state_dict(),\n",
    "                    'optimizer_state': optimizer.state_dict()},\n",
    "                    os.path.join(args.output_dir, 'model_checkpoint.pt'))\n",
    "        # save model only\n",
    "        torch.save(model.state_dict(), os.path.join(args.output_dir, 'model_state.pt'))\n",
    "\n",
    "        # save best state\n",
    "        if eval_logprob > best_eval_logprob:\n",
    "            best_eval_logprob = eval_logprob\n",
    "            torch.save({'epoch': i,\n",
    "                        'model_state': model.state_dict(),\n",
    "                        'optimizer_state': optimizer.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'best_model_checkpoint.pt'))\n",
    "\n",
    "        # plot sample\n",
    "        if args.dataset == 'TOY':\n",
    "            plot_sample_and_density(model, train_loader.dataset.base_dist, args, step=i)\n",
    "        if args.dataset == 'MNIST':\n",
    "            generate(model, train_loader.dataset.lam, args, step=i)\n",
    "\n",
    "# --------------------\n",
    "# Plot\n",
    "# --------------------\n",
    "\n",
    "def plot_density(dist, ax, ranges, flip_var_order=False):\n",
    "    (xmin, xmax), (ymin, ymax) = ranges\n",
    "    # sample uniform grid\n",
    "    n = 200\n",
    "    xx1 = torch.linspace(xmin, xmax, n)\n",
    "    xx2 = torch.linspace(ymin, ymax, n)\n",
    "    xx, yy = torch.meshgrid(xx1, xx2)\n",
    "    xy = torch.stack((xx.flatten(), yy.flatten()), dim=-1).squeeze()\n",
    "\n",
    "    if flip_var_order:\n",
    "        xy = xy.flip(1)\n",
    "\n",
    "    # run uniform grid through model and plot\n",
    "    density = dist.log_prob(xy).exp()\n",
    "    ax.contour(xx, yy, density.view(n,n).data.numpy())\n",
    "\n",
    "    # format\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    ax.set_xticks([xmin, xmax])\n",
    "    ax.set_yticks([ymin, ymax])\n",
    "\n",
    "def plot_dist_sample(data, ax, ranges):\n",
    "    ax.scatter(data[:,0].data.numpy(), data[:,1].data.numpy(), s=10, alpha=0.4)\n",
    "    # format\n",
    "    (xmin, xmax), (ymin, ymax) = ranges\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    ax.set_xticks([xmin, xmax])\n",
    "    ax.set_yticks([ymin, ymax])\n",
    "\n",
    "def plot_sample_and_density(model, target_dist, args, ranges_density=[[-5,20],[-10,10]], ranges_sample=[[-4,4],[-4,4]], step=None):\n",
    "    model.eval()\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(6,3))\n",
    "\n",
    "    # sample target distribution and pass through model\n",
    "    data = target_dist.sample((2000,))\n",
    "    u, _ = model(data)\n",
    "\n",
    "    # plot density and sample\n",
    "    plot_density(model, axs[0], ranges_density, args.flip_var_order)\n",
    "    plot_dist_sample(u, axs[1], ranges_sample)\n",
    "\n",
    "    # format and save\n",
    "    matplotlib.rcParams.update({'xtick.labelsize': 'xx-small', 'ytick.labelsize': 'xx-small'})\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(args.output_dir, 'sample' + (step != None)*'_epoch_{}'.format(step) + '.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Run\n",
    "# --------------------\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # setup file ops\n",
    "    if not os.path.isdir(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "\n",
    "    # setup device\n",
    "    args.device = torch.device('cuda:0' if torch.cuda.is_available() and not args.no_cuda else 'cpu')\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.device.type == 'cuda': torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "    # load data\n",
    "    if args.conditional: assert args.dataset in ['MNIST', 'CIFAR10'], 'Conditional inputs only available for labeled datasets MNIST and CIFAR10.'\n",
    "    train_dataloader, test_dataloader = fetch_dataloaders(args.dataset, args.batch_size, args.device, args.flip_toy_var_order)\n",
    "    args.input_size = train_dataloader.dataset.input_size\n",
    "    args.input_dims = train_dataloader.dataset.input_dims\n",
    "    args.cond_label_size = train_dataloader.dataset.label_size if args.conditional else None\n",
    "\n",
    "    model = RealNVP(args.n_blocks, args.input_size, args.hidden_size, args.n_hidden, args.cond_label_size,\n",
    "                    batch_norm=not args.no_batch_norm)\n",
    "\n",
    "    model = model.to(args.device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-6)\n",
    "\n",
    "    if args.restore_file:\n",
    "        # load model and optimizer states\n",
    "        state = torch.load(args.restore_file, map_location=args.device)\n",
    "        model.load_state_dict(state['model_state'])\n",
    "        optimizer.load_state_dict(state['optimizer_state'])\n",
    "        args.start_epoch = state['epoch'] + 1\n",
    "        # set up paths\n",
    "        args.output_dir = os.path.dirname(args.restore_file)\n",
    "    args.results_file = os.path.join(args.output_dir, args.results_file)\n",
    "\n",
    "    print('Loaded settings and model:')\n",
    "    print(pprint.pformat(args.__dict__))\n",
    "    print(model)\n",
    "    print(pprint.pformat(args.__dict__), file=open(args.results_file, 'a'))\n",
    "    print(model, file=open(args.results_file, 'a'))\n",
    "\n",
    "    if args.train:\n",
    "        train_and_evaluate(model, train_dataloader, test_dataloader, optimizer, args)\n",
    "\n",
    "    if args.evaluate:\n",
    "        evaluate(model, test_dataloader, None, args)\n",
    "\n",
    "    if args.generate:\n",
    "        if args.dataset == 'TOY':\n",
    "            base_dist = train_dataloader.dataset.base_dist\n",
    "            plot_sample_and_density(model, base_dist, args, ranges_density=[[-15,4],[-3,3]], ranges_sample=[[-1.5,1.5],[-3,3]])\n",
    "        elif args.dataset == 'MNIST':\n",
    "            generate(model, train_dataloader.dataset.lam, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
