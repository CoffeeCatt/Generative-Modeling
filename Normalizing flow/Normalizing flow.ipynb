{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motivation: an ideal family of variational distributions $q_\\phi(z|x)$ should be highly flexible.\n",
    "\n",
    "Langevin flow:\n",
    "\n",
    "$$dz(t) = F(z(t),t)dt +G(z(t),t)d\\xi(t)$$\n",
    "\n",
    "$F(z,t) = -\\nabla_z L(z)$,$G(z,t)=\\sqrt{2}\\delta_{ij}$, where $L(z)$ is an unnormalized log-density of our model.\n",
    "\n",
    "$q_\\infty(z)\\propto e^{-L(z)}$, the true posterior.\n",
    "\n",
    "Planar transformation:\n",
    "\n",
    "$$f(z) = z+uh(w^\\top z+b)$$\n",
    "$$\\det\\left|\\frac{\\partial f}{\\partial z}\\right| = |1+u^\\top \\psi(z)|,$$ where $\\psi(z)= h'(w^\\top z +b)w$.\n",
    "\n",
    "$$\\ln q_K(z_K)=\\ln q_0(z)-\\sum_{k=1}^K\\ln \\left|1+u_k^\\top \\psi_k(z_k)\\right|.$$\n",
    "\n",
    "Annealed version:\n",
    "$$F^{\\beta_t}(x) = \\mathbb{E}_{q_0(z_0)}[\\ln q_0(z_0)]-\\beta_t \\mathbb{E}_{q_0}[\\log p(x,z_K)]-\\mathbb{E}_{q_{0}(z_0)}\\left[\\sum_{k=1}^K\\ln|1+u_k^\\top \\psi_k(z_k)|\\right].$$\n",
    "where $\\beta \\in [0,1]$ is an inverse temperature that follows a schedule $\\beta_t=\\min(1,0.01+t/10000)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/kamenbliznashki/normalizing_flows/blob/master/planar_flow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     56,
     58,
     78,
     160,
     189,
     209
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Variational Inference with Normalizing Flows\n",
    "arXiv:1505.05770v6\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as D\n",
    "\n",
    "import math\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# action\n",
    "parser.add_argument('--train', action='store_true', help='Train a flow.')\n",
    "parser.add_argument('--evaluate', action='store_true', help='Evaluate a flow.')\n",
    "parser.add_argument('--plot', action='store_true', help='Plot a flow and target density.')\n",
    "parser.add_argument('--restore_file', type=str, help='Path to model to restore.')\n",
    "parser.add_argument('--output_dir', default='.', help='Path to output folder.')\n",
    "parser.add_argument('--no_cuda', action='store_true', help='Do not use cuda.')\n",
    "# target potential\n",
    "parser.add_argument('--target_potential', choices=['u_z0', 'u_z5', 'u_z1', 'u_z2', 'u_z3', 'u_z4'], help='Which potential function to approximate.')\n",
    "# flow params\n",
    "parser.add_argument('--base_sigma', type=float, default=4, help='Std of the base isotropic 0-mean Gaussian distribution.')\n",
    "parser.add_argument('--learn_base', default=False, action='store_true', help='Whether to learn a mu-sigma affine transform of the base distribution.')\n",
    "parser.add_argument('--flow_length', type=int, default=2, help='Length of the flow.')\n",
    "# training params\n",
    "parser.add_argument('--init_sigma', type=float, default=1, help='Initialization std for the trainable flow parameters.')\n",
    "parser.add_argument('--batch_size', type=int, default=100)\n",
    "parser.add_argument('--start_step', type=int, default=0, help='Starting step (if resuming training will be overwrite from filename).')\n",
    "parser.add_argument('--n_steps', type=int, default=1000000, help='Optimization steps.')\n",
    "parser.add_argument('--lr', type=float, default=1e-5, help='Learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-3, help='Weight decay.')\n",
    "parser.add_argument('--beta', type=float, default=1, help='Multiplier for the target potential loss.')\n",
    "parser.add_argument('--seed', type=int, default=2, help='Random seed.')\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Flow\n",
    "# --------------------\n",
    "\n",
    "class PlanarTransform(nn.Module):\n",
    "    def __init__(self, init_sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.u = nn.Parameter(torch.randn(1, 2).normal_(0, init_sigma))\n",
    "        self.w = nn.Parameter(torch.randn(1, 2).normal_(0, init_sigma))\n",
    "        self.b = nn.Parameter(torch.randn(1).fill_(0))\n",
    "\n",
    "    def forward(self, x, normalize_u=True):\n",
    "        # allow for a single forward pass over all the transforms in the flows with a Sequential container\n",
    "        if isinstance(x, tuple):\n",
    "            z, sum_log_abs_det_jacobians = x\n",
    "        else:\n",
    "            z, sum_log_abs_det_jacobians = x, 0\n",
    "\n",
    "        # normalize u s.t. w @ u >= -1; sufficient condition for invertibility\n",
    "        u_hat = self.u\n",
    "        if normalize_u:\n",
    "            wtu = (self.w @ self.u.t()).squeeze()\n",
    "            m_wtu = - 1 + torch.log1p(wtu.exp())\n",
    "            u_hat = self.u + (m_wtu - wtu) * self.w / (self.w @ self.w.t())\n",
    "\n",
    "        # compute transform\n",
    "        f_z = z + u_hat * torch.tanh(z @ self.w.t() + self.b)\n",
    "        # compute log_abs_det_jacobian\n",
    "        psi = (1 - torch.tanh(z @ self.w.t() + self.b)**2) @ self.w\n",
    "        det = 1 + psi @ u_hat.t()\n",
    "        log_abs_det_jacobian = torch.log(torch.abs(det) + 1e-6).squeeze()\n",
    "        sum_log_abs_det_jacobians = sum_log_abs_det_jacobians + log_abs_det_jacobian\n",
    "\n",
    "        return f_z, sum_log_abs_det_jacobians\n",
    "\n",
    "class AffineTransform(nn.Module):\n",
    "    def __init__(self, learnable=False):\n",
    "        super().__init__()\n",
    "        self.mu = nn.Parameter(torch.zeros(2)).requires_grad_(learnable)\n",
    "        self.logsigma = nn.Parameter(torch.zeros(2)).requires_grad_(learnable)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.mu + self.logsigma.exp() * x\n",
    "        sum_log_abs_det_jacobians = self.logsigma.sum()\n",
    "        return z, sum_log_abs_det_jacobians\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Test energy functions -- NF paper table 1\n",
    "# --------------------\n",
    "\n",
    "w1 = lambda z: torch.sin(2 * math.pi * z[:,0] / 4)\n",
    "w2 = lambda z: 3 * torch.exp(-0.5 * ((z[:,0] - 1)/0.6)**2)\n",
    "w3 = lambda z: 3 * torch.sigmoid((z[:,0] - 1) / 0.3)\n",
    "\n",
    "u_z1 = lambda z: 0.5 * ((torch.norm(z, p=2, dim=1) - 2) / 0.4)**2 - \\\n",
    "                 torch.log(torch.exp(-0.5*((z[:,0] - 2) / 0.6)**2) + torch.exp(-0.5*((z[:,0] + 2) / 0.6)**2) + 1e-10)\n",
    "u_z2 = lambda z: 0.5 * ((z[:,1] - w1(z)) / 0.4)**2\n",
    "u_z3 = lambda z: - torch.log(torch.exp(-0.5*((z[:,1] - w1(z))/0.35)**2) + torch.exp(-0.5*((z[:,1] - w1(z) + w2(z))/0.35)**2) + 1e-10)\n",
    "u_z4 = lambda z: - torch.log(torch.exp(-0.5*((z[:,1] - w1(z))/0.4)**2) + torch.exp(-0.5*((z[:,1] - w1(z) + w3(z))/0.35)**2) + 1e-10)\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Training\n",
    "# --------------------\n",
    "\n",
    "def optimize_flow(base_dist, flow, target_energy_potential, optimizer, args):\n",
    "\n",
    "    # anneal rate for free energy\n",
    "    temp = lambda i: min(1, 0.01 + i/10000)\n",
    "\n",
    "    for i in range(args.start_step, args.n_steps):\n",
    "\n",
    "        # sample base dist\n",
    "        z = base_dist.sample((args.batch_size, )).to(args.device)\n",
    "\n",
    "        # pass through flow:\n",
    "        # 1. compute expected log_prob of data under base dist -- nothing tied to parameters here so irrelevant to grads\n",
    "        base_log_prob = base_dist.log_prob(z)\n",
    "        # 2. compute sum of log_abs_det_jacobian through the flow\n",
    "        zk, sum_log_abs_det_jacobians = flow(z)\n",
    "        # 3. compute expected log_prob of z_k the target_energy potential\n",
    "        p_log_prob = - temp(i) * target_energy_potential(zk)  # p = exp(-potential) ==> p_log_prob = - potential\n",
    "\n",
    "        loss = base_log_prob - sum_log_abs_det_jacobians - args.beta * p_log_prob\n",
    "        loss = loss.mean(0)\n",
    "\n",
    "        # compute loss and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            # display loss\n",
    "            log_qk = base_dist.log_prob(z) - sum_log_abs_det_jacobians\n",
    "            print('{}: step {:5d} / {}; loss {:.3f}; base_log_prob {:.3f}, sum log dets {:.3f}, p_log_prob {:.3f}, max base = {:.3f}; max qk = {:.3f} \\\n",
    "                zk_mean {}, zk_sigma {}; base_mu {}, base_log_sigma {}'.format(\n",
    "                args.target_potential, i, args.n_steps, loss.item(), base_log_prob.mean(0).item(), sum_log_abs_det_jacobians.mean(0).item(),\n",
    "                p_log_prob.mean(0).item(), base_log_prob.exp().max().item(), log_qk.exp().max().item(),\n",
    "                zk.mean(0).cpu().data.numpy(), zk.var(0).sqrt().cpu().data.numpy(),\n",
    "                base_dist.loc.cpu().data.numpy() if not args.learn_base else flow[0].mu.cpu().data.numpy(),\n",
    "                base_dist.covariance_matrix.cpu().diag().data.numpy() if not args.learn_base else flow[0].logsigma.cpu().data.numpy()))\n",
    "\n",
    "            # save model\n",
    "            torch.save({'step': i,\n",
    "                        'flow_state': flow.state_dict(),\n",
    "                        'optimizer_state': optimizer.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'model_state_flow_length_{}.pt'.format(args.flow_length)))\n",
    "\n",
    "            # plot and save results\n",
    "            with torch.no_grad():\n",
    "                plot_flow(base_dist, flow, os.path.join(args.output_dir, 'approximating_flow_step{}.png'.format(i)), args)\n",
    "\n",
    "# --------------------\n",
    "# Plotting\n",
    "# --------------------\n",
    "\n",
    "def plot_flow(base_dist, flow, filename, args):\n",
    "    n = 200\n",
    "    lim = 4\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, subplot_kw={'aspect': 'equal'})\n",
    "\n",
    "    # plot target density we're trying to approx\n",
    "    plot_target_density(u_z, axs[0,0], lim, n)\n",
    "\n",
    "    # plot posterior approx density\n",
    "    plot_flow_density(base_dist, flow, axs[0,1], lim, n)\n",
    "\n",
    "    # plot flow-transformed base dist sample and histogram\n",
    "    z = base_dist.sample((10000,))\n",
    "    zk, _ = flow(z)\n",
    "    zk = zk.cpu().data.numpy()\n",
    "    axs[1,0].scatter(zk[:,0], zk[:,1], s=10, alpha=0.4)\n",
    "    axs[1,1].hist2d(zk[:,0], zk[:,1], bins=lim*50, cmap=plt.cm.jet)\n",
    "\n",
    "    for ax in plt.gcf().axes:\n",
    "        ax.get_xaxis().set_visible(True)\n",
    "        ax.get_yaxis().set_visible(True)\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_target_density(u_z, ax, range_lim=4, n=200, output_dir=None):\n",
    "    x = torch.linspace(-range_lim, range_lim, n)\n",
    "    xx, yy = torch.meshgrid((x, x))\n",
    "    zz = torch.stack((xx.flatten(), yy.flatten()), dim=-1).squeeze().to(args.device)\n",
    "\n",
    "    ax.pcolormesh(xx, yy, torch.exp(-u_z(zz)).view(n,n).data, cmap=plt.cm.jet)\n",
    "\n",
    "    for ax in plt.gcf().axes:\n",
    "        ax.set_xlim(-range_lim, range_lim)\n",
    "        ax.set_ylim(-range_lim, range_lim)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "    if output_dir:\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'target_potential_density.png'))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def plot_flow_density(base_dist, flow, ax, range_lim=4, n=200, output_dir=None):\n",
    "    x = torch.linspace(-range_lim, range_lim, n)\n",
    "    xx, yy = torch.meshgrid((x, x))\n",
    "    zz = torch.stack((xx.flatten(), yy.flatten()), dim=-1).squeeze().to(args.device)\n",
    "\n",
    "    # plot posterior approx density\n",
    "    zzk, sum_log_abs_det_jacobians = flow(zz)\n",
    "    log_q0 = base_dist.log_prob(zz)\n",
    "    log_qk = log_q0 - sum_log_abs_det_jacobians\n",
    "    qk = log_qk.exp().cpu()\n",
    "    zzk = zzk.cpu()\n",
    "    ax.pcolormesh(zzk[:,0].view(n,n).data, zzk[:,1].view(n,n).data, qk.view(n,n).data, cmap=plt.cm.jet)\n",
    "    ax.set_facecolor(plt.cm.jet(0.))\n",
    "\n",
    "    for ax in plt.gcf().axes:\n",
    "        ax.set_xlim(-range_lim, range_lim)\n",
    "        ax.set_ylim(-range_lim, range_lim)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "    if output_dir:\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'flow_k{}_density.png'.format(len(flow)-1)))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Run\n",
    "# --------------------\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    args.device = torch.device('cuda:0' if torch.cuda.is_available() and not args.no_cuda else 'cpu')\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.device.type == 'cuda': torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "    # setup flow\n",
    "    flow = nn.Sequential(AffineTransform(args.learn_base), *[PlanarTransform() for _ in range(args.flow_length)]).to(args.device)\n",
    "\n",
    "    # setup target potential to approx\n",
    "    u_z = vars()[args.target_potential]\n",
    "\n",
    "    # setup base distribution\n",
    "    base_dist = D.MultivariateNormal(torch.zeros(2).to(args.device), args.base_sigma * torch.eye(2).to(args.device))\n",
    "\n",
    "    if args.restore_file:\n",
    "        # get filename\n",
    "        filename = os.path.basename(args.restore_file)\n",
    "        args.flow_length = int(filename.partition('length_')[-1].rpartition('.')[0])\n",
    "        # reset output dir\n",
    "        args.output_dir = os.path.dirname(args.restore_file)\n",
    "        # load state\n",
    "        state = torch.load(args.restore_file, map_location=args.device)\n",
    "        # compatibility code;\n",
    "        # 1/ earlier models did not include step and optimizer checkpoints;\n",
    "        try:\n",
    "            flow_state = state['flow_state']\n",
    "            optimizer_state = state['optimizer_state']\n",
    "            args.start_step = state['step']\n",
    "        except KeyError:\n",
    "            # if state is not a dict, load just the model state\n",
    "            flow_state = state\n",
    "            optimizer_state = None\n",
    "        # 2/ some saved checkpoints may not have a first affine layer\n",
    "        try:\n",
    "            flow_state['0.mu']\n",
    "        except KeyError:\n",
    "            # if no first affine layer, reload a flow model without one\n",
    "            flow = nn.Sequential(*[PlanarTransform(args.init_sigma) for _ in range(args.flow_length)])\n",
    "        flow.load_state_dict(flow_state)\n",
    "\n",
    "    if not os.path.isdir(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "\n",
    "    if args.train:\n",
    "        optimizer = torch.optim.RMSprop(flow.parameters(), lr=args.lr, momentum=0.9, alpha=0.90, eps=1e-6, weight_decay=args.weight_decay)\n",
    "        if args.restore_file and optimizer_state:\n",
    "            optimizer.load_state_dict(optimizer_state)\n",
    "        args.n_steps = args.start_step + args.n_steps\n",
    "        optimize_flow(base_dist, flow, u_z, optimizer, args)\n",
    "\n",
    "    if args.evaluate:\n",
    "        plot_flow(base_dist, flow, os.path.join(args.output_dir, 'approximating_flow.png'), args)\n",
    "\n",
    "    if args.plot:\n",
    "        plot_target_density(u_z, plt.gca(), output_dir=args.output_dir)\n",
    "        plot_flow_density(base_dist, flow, plt.gca(), output_dir=args.output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
