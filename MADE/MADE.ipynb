{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoregressive property: $\\hat{x}_d$ only depends on the previous input units $x_{<d}$, $\\hat{x}_d = p(x_d|x_{<d})$. \n",
    "\n",
    "$$M_{d',d}^{V,W}=\\sum_{k=1}^K \\mathbb{1}_{d'>m(k)}\\mathbb{1}_{m(k)\\geq d'}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/kamenbliznashki/normalizing_flows/blob/master/maf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     91,
     99,
     140,
     142,
     144,
     150,
     156,
     167,
     178,
     183,
     224,
     227,
     272,
     296,
     336,
     367,
     400,
     422,
     431,
     455
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Masked Autoregressive Flow for Density Estimation\n",
    "arXiv:1705.07057v4\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import math\n",
    "import argparse\n",
    "import pprint\n",
    "import copy\n",
    "\n",
    "from data import fetch_dataloaders\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# action\n",
    "parser.add_argument('--train', action='store_true', help='Train a flow.')\n",
    "parser.add_argument('--evaluate', action='store_true', help='Evaluate a flow.')\n",
    "parser.add_argument('--restore_file', type=str, help='Path to model to restore.')\n",
    "parser.add_argument('--generate', action='store_true', help='Generate samples from a model.')\n",
    "parser.add_argument('--data_dir', default='./data/', help='Location of datasets.')\n",
    "parser.add_argument('--output_dir', default='./results/{}'.format(os.path.splitext(__file__)[0]))\n",
    "parser.add_argument('--results_file', default='results.txt', help='Filename where to store settings and test results.')\n",
    "parser.add_argument('--no_cuda', action='store_true', help='Do not use cuda.')\n",
    "# data\n",
    "parser.add_argument('--dataset', default='toy', help='Which dataset to use.')\n",
    "parser.add_argument('--flip_toy_var_order', action='store_true', help='Whether to flip the toy dataset variable order to (x2, x1).')\n",
    "parser.add_argument('--seed', type=int, default=1, help='Random seed to use.')\n",
    "# model\n",
    "parser.add_argument('--model', default='maf', help='Which model to use: made, maf.')\n",
    "# made parameters\n",
    "parser.add_argument('--n_blocks', type=int, default=5, help='Number of blocks to stack in a model (MADE in MAF; Coupling+BN in RealNVP).')\n",
    "parser.add_argument('--n_components', type=int, default=1, help='Number of Gaussian clusters for mixture of gaussians models.')\n",
    "parser.add_argument('--hidden_size', type=int, default=100, help='Hidden layer size for MADE (and each MADE block in an MAF).')\n",
    "parser.add_argument('--n_hidden', type=int, default=1, help='Number of hidden layers in each MADE.')\n",
    "parser.add_argument('--activation_fn', type=str, default='relu', help='What activation function to use in the MADEs.')\n",
    "parser.add_argument('--input_order', type=str, default='sequential', help='What input order to use (sequential | random).')\n",
    "parser.add_argument('--conditional', default=False, action='store_true', help='Whether to use a conditional model.')\n",
    "parser.add_argument('--no_batch_norm', action='store_true')\n",
    "# training params\n",
    "parser.add_argument('--batch_size', type=int, default=100)\n",
    "parser.add_argument('--n_epochs', type=int, default=50)\n",
    "parser.add_argument('--start_epoch', default=0, help='Starting epoch (for logging; to be overwritten when restoring file.')\n",
    "parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate.')\n",
    "parser.add_argument('--log_interval', type=int, default=1000, help='How often to show loss statistics and save samples.')\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Model layers and helpers\n",
    "# --------------------\n",
    "\n",
    "def create_masks(input_size, hidden_size, n_hidden, input_order='sequential', input_degrees=None):\n",
    "    # MADE paper sec 4:\n",
    "    # degrees of connections between layers -- ensure at most in_degree - 1 connections\n",
    "    degrees = []\n",
    "\n",
    "    # set input degrees to what is provided in args (the flipped order of the previous layer in a stack of mades);\n",
    "    # else init input degrees based on strategy in input_order (sequential or random)\n",
    "    if input_order == 'sequential':\n",
    "        degrees += [torch.arange(input_size)] if input_degrees is None else [input_degrees]\n",
    "        for _ in range(n_hidden + 1):\n",
    "            degrees += [torch.arange(hidden_size) % (input_size - 1)]\n",
    "        degrees += [torch.arange(input_size) % input_size - 1] if input_degrees is None else [input_degrees % input_size - 1]\n",
    "\n",
    "    elif input_order == 'random':\n",
    "        degrees += [torch.randperm(input_size)] if input_degrees is None else [input_degrees]\n",
    "        for _ in range(n_hidden + 1):\n",
    "            min_prev_degree = min(degrees[-1].min().item(), input_size - 1)\n",
    "            degrees += [torch.randint(min_prev_degree, input_size, (hidden_size,))]\n",
    "        min_prev_degree = min(degrees[-1].min().item(), input_size - 1)\n",
    "        degrees += [torch.randint(min_prev_degree, input_size, (input_size,)) - 1] if input_degrees is None else [input_degrees - 1]\n",
    "\n",
    "    # construct masks\n",
    "    masks = []\n",
    "    for (d0, d1) in zip(degrees[:-1], degrees[1:]):\n",
    "        masks += [(d1.unsqueeze(-1) >= d0.unsqueeze(0)).float()]\n",
    "\n",
    "    return masks, degrees[0]\n",
    "\n",
    "\n",
    "class MaskedLinear(nn.Linear):\n",
    "    \"\"\" MADE building block layer \"\"\"\n",
    "    def __init__(self, input_size, n_outputs, mask, cond_label_size=None):\n",
    "        super().__init__(input_size, n_outputs)\n",
    "\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "        self.cond_label_size = cond_label_size\n",
    "        if cond_label_size is not None:\n",
    "            self.cond_weight = nn.Parameter(torch.rand(n_outputs, cond_label_size) / math.sqrt(cond_label_size))\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        out = F.linear(x, self.weight * self.mask, self.bias)\n",
    "        if y is not None:\n",
    "            out = out + F.linear(y, self.cond_weight)\n",
    "        return out\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        ) + (self.cond_label_size != None) * ', cond_features={}'.format(self.cond_label_size)\n",
    "\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Models\n",
    "# --------------------\n",
    "\n",
    "class MADE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_hidden, cond_label_size=None, activation='relu', input_order='sequential', input_degrees=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size -- scalar; dim of inputs\n",
    "            hidden_size -- scalar; dim of hidden layers\n",
    "            n_hidden -- scalar; number of hidden layers\n",
    "            activation -- str; activation function to use\n",
    "            input_order -- str or tensor; variable order for creating the autoregressive masks (sequential|random)\n",
    "                            or the order flipped from the previous layer in a stack of mades\n",
    "            conditional -- bool; whether model is conditional\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # base distribution for calculation of log prob under the model\n",
    "        self.register_buffer('base_dist_mean', torch.zeros(input_size))\n",
    "        self.register_buffer('base_dist_var', torch.ones(input_size))\n",
    "\n",
    "        # create masks\n",
    "        masks, self.input_degrees = create_masks(input_size, hidden_size, n_hidden, input_order, input_degrees)\n",
    "\n",
    "        # setup activation\n",
    "        if activation == 'relu':\n",
    "            activation_fn = nn.ReLU()\n",
    "        elif activation == 'tanh':\n",
    "            activation_fn = nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError('Check activation function.')\n",
    "\n",
    "        # construct model\n",
    "        self.net_input = MaskedLinear(input_size, hidden_size, masks[0], cond_label_size)\n",
    "        self.net = []\n",
    "        for m in masks[1:-1]:\n",
    "            self.net += [activation_fn, MaskedLinear(hidden_size, hidden_size, m)]\n",
    "        self.net += [activation_fn, MaskedLinear(hidden_size, 2 * input_size, masks[-1].repeat(2,1))]\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "\n",
    "    @property\n",
    "    def base_dist(self):\n",
    "        return D.Normal(self.base_dist_mean, self.base_dist_var)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        # MAF eq 4 -- return mean and log std\n",
    "        m, loga = self.net(self.net_input(x, y)).chunk(chunks=2, dim=1)\n",
    "        u = (x - m) * torch.exp(-loga)\n",
    "        # MAF eq 5\n",
    "        log_abs_det_jacobian = - loga\n",
    "        return u, log_abs_det_jacobian\n",
    "\n",
    "    def inverse(self, u, y=None, sum_log_abs_det_jacobians=None):\n",
    "        # MAF eq 3\n",
    "        D = u.shape[1]\n",
    "        x = torch.zeros_like(u)\n",
    "        # run through reverse model\n",
    "        for i in self.input_degrees:\n",
    "            m, loga = self.net(self.net_input(x, y)).chunk(chunks=2, dim=1)\n",
    "            x[:,i] = u[:,i] * torch.exp(loga[:,i]) + m[:,i]\n",
    "        log_abs_det_jacobian = loga\n",
    "        return x, log_abs_det_jacobian\n",
    "\n",
    "    def log_prob(self, x, y=None):\n",
    "        u, log_abs_det_jacobian = self.forward(x, y)\n",
    "        return torch.sum(self.base_dist.log_prob(u) + log_abs_det_jacobian, dim=1)\n",
    "\n",
    "\n",
    "class MADEMOG(nn.Module):\n",
    "    \"\"\" Mixture of Gaussians MADE \"\"\"\n",
    "    def __init__(self, n_components, input_size, hidden_size, n_hidden, cond_label_size=None, activation='relu', input_order='sequential', input_degrees=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_components -- scalar; number of gauassian components in the mixture\n",
    "            input_size -- scalar; dim of inputs\n",
    "            hidden_size -- scalar; dim of hidden layers\n",
    "            n_hidden -- scalar; number of hidden layers\n",
    "            activation -- str; activation function to use\n",
    "            input_order -- str or tensor; variable order for creating the autoregressive masks (sequential|random)\n",
    "                            or the order flipped from the previous layer in a stack of mades\n",
    "            conditional -- bool; whether model is conditional\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_components = n_components\n",
    "\n",
    "        # base distribution for calculation of log prob under the model\n",
    "        self.register_buffer('base_dist_mean', torch.zeros(input_size))\n",
    "        self.register_buffer('base_dist_var', torch.ones(input_size))\n",
    "\n",
    "        # create masks\n",
    "        masks, self.input_degrees = create_masks(input_size, hidden_size, n_hidden, input_order, input_degrees)\n",
    "\n",
    "        # setup activation\n",
    "        if activation == 'relu':\n",
    "            activation_fn = nn.ReLU()\n",
    "        elif activation == 'tanh':\n",
    "            activation_fn = nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError('Check activation function.')\n",
    "\n",
    "        # construct model\n",
    "        self.net_input = MaskedLinear(input_size, hidden_size, masks[0], cond_label_size)\n",
    "        self.net = []\n",
    "        for m in masks[1:-1]:\n",
    "            self.net += [activation_fn, MaskedLinear(hidden_size, hidden_size, m)]\n",
    "        self.net += [activation_fn, MaskedLinear(hidden_size, n_components * 3 * input_size, masks[-1].repeat(n_components * 3,1))]\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "\n",
    "    @property\n",
    "    def base_dist(self):\n",
    "        return D.Normal(self.base_dist_mean, self.base_dist_var)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        # shapes\n",
    "        N, L = x.shape\n",
    "        C = self.n_components\n",
    "        # MAF eq 2 -- parameters of Gaussians - mean, logsigma, log unnormalized cluster probabilities\n",
    "        m, loga, logr = self.net(self.net_input(x, y)).view(N, C, 3 * L).chunk(chunks=3, dim=-1)  # out 3 x (N, C, L)\n",
    "        # MAF eq 4\n",
    "        x = x.repeat(1, C).view(N, C, L)  # out (N, C, L)\n",
    "        u = (x - m) * torch.exp(-loga)  # out (N, C, L)\n",
    "        # MAF eq 5\n",
    "        log_abs_det_jacobian = - loga  # out (N, C, L)\n",
    "        # normalize cluster responsibilities\n",
    "        self.logr = logr - logr.logsumexp(1, keepdim=True)  # out (N, C, L)\n",
    "        return u, log_abs_det_jacobian\n",
    "\n",
    "    def inverse(self, u, y=None, sum_log_abs_det_jacobians=None):\n",
    "        # shapes\n",
    "        N, C, L = u.shape\n",
    "        # init output\n",
    "        x = torch.zeros(N, L).to(u.device)\n",
    "        # MAF eq 3\n",
    "        # run through reverse model along each L\n",
    "        for i in self.input_degrees:\n",
    "            m, loga, logr = self.net(self.net_input(x, y)).view(N, C, 3 * L).chunk(chunks=3, dim=-1)  # out 3 x (N, C, L)\n",
    "            # normalize cluster responsibilities and sample cluster assignments from a categorical dist\n",
    "            logr = logr - logr.logsumexp(1, keepdim=True)  # out (N, C, L)\n",
    "            z = D.Categorical(logits=logr[:,:,i]).sample().unsqueeze(-1)  # out (N, 1)\n",
    "            u_z = torch.gather(u[:,:,i], 1, z).squeeze()  # out (N, 1)\n",
    "            m_z = torch.gather(m[:,:,i], 1, z).squeeze()  # out (N, 1)\n",
    "            loga_z = torch.gather(loga[:,:,i], 1, z).squeeze()\n",
    "            x[:,i] = u_z * torch.exp(loga_z) + m_z\n",
    "        log_abs_det_jacobian = loga\n",
    "        return x, log_abs_det_jacobian\n",
    "\n",
    "    def log_prob(self, x, y=None):\n",
    "        u, log_abs_det_jacobian = self.forward(x, y)  # u = (N,C,L); log_abs_det_jacobian = (N,C,L)\n",
    "        # marginalize cluster probs\n",
    "        log_probs = torch.logsumexp(self.logr + self.base_dist.log_prob(u) + log_abs_det_jacobian, dim=1)  # sum over C; out (N, L)\n",
    "        return log_probs.sum(1)  # sum over L; out (N,)\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Train and evaluate\n",
    "# --------------------\n",
    "\n",
    "def train(model, dataloader, optimizer, epoch, args):\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "        model.train()\n",
    "\n",
    "        # check if labeled dataset\n",
    "        if len(data) == 1:\n",
    "            x, y = data[0], None\n",
    "        else:\n",
    "            x, y = data\n",
    "            y = y.to(args.device)\n",
    "        x = x.view(x.shape[0], -1).to(args.device)\n",
    "\n",
    "        loss = - model.log_prob(x, y if args.cond_label_size else None).mean(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % args.log_interval == 0:\n",
    "            print('epoch {:3d} / {}, step {:4d} / {}; loss {:.4f}'.format(\n",
    "                epoch, args.start_epoch + args.n_epochs, i, len(dataloader), loss.item()))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, epoch, args):\n",
    "    model.eval()\n",
    "\n",
    "    # conditional model\n",
    "    if args.cond_label_size is not None:\n",
    "        logprior = torch.tensor(1 / args.cond_label_size).log().to(args.device)\n",
    "        loglike = [[] for _ in range(args.cond_label_size)]\n",
    "\n",
    "        for i in range(args.cond_label_size):\n",
    "            # make one-hot labels\n",
    "            labels = torch.zeros(args.batch_size, args.cond_label_size).to(args.device)\n",
    "            labels[:,i] = 1\n",
    "\n",
    "            for x, y in dataloader:\n",
    "                x = x.view(x.shape[0], -1).to(args.device)\n",
    "                loglike[i].append(model.log_prob(x, labels))\n",
    "\n",
    "            loglike[i] = torch.cat(loglike[i], dim=0)   # cat along data dim under this label\n",
    "        loglike = torch.stack(loglike, dim=1)           # cat all data along label dim\n",
    "\n",
    "        # log p(x) = log ∑_y p(x,y) = log ∑_y p(x|y)p(y)\n",
    "        # assume uniform prior      = log p(y) ∑_y p(x|y) = log p(y) + log ∑_y p(x|y)\n",
    "        logprobs = logprior + loglike.logsumexp(dim=1)\n",
    "        # TODO -- measure accuracy as argmax of the loglike\n",
    "\n",
    "    # unconditional model\n",
    "    else:\n",
    "        logprobs = []\n",
    "        for data in dataloader:\n",
    "            x = data[0].view(data[0].shape[0], -1).to(args.device)\n",
    "            logprobs.append(model.log_prob(x))\n",
    "        logprobs = torch.cat(logprobs, dim=0).to(args.device)\n",
    "\n",
    "    logprob_mean, logprob_std = logprobs.mean(0), 2 * logprobs.var(0).sqrt() / math.sqrt(len(dataloader.dataset))\n",
    "    output = 'Evaluate ' + (epoch != None)*'(epoch {}) -- '.format(epoch) + 'logp(x) = {:.3f} +/- {:.3f}'.format(logprob_mean, logprob_std)\n",
    "    print(output)\n",
    "    print(output, file=open(args.results_file, 'a'))\n",
    "    return logprob_mean, logprob_std\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, dataset_lam, args, step=None, n_row=10):\n",
    "    model.eval()\n",
    "\n",
    "    # conditional model\n",
    "    if args.cond_label_size:\n",
    "        samples = []\n",
    "        labels = torch.eye(args.cond_label_size).to(args.device)\n",
    "\n",
    "        for i in range(args.cond_label_size):\n",
    "            # sample model base distribution and run through inverse model to sample data space\n",
    "            u = model.base_dist.sample((n_row, args.n_components)).squeeze()\n",
    "            labels_i = labels[i].expand(n_row, -1)\n",
    "            sample, _ = model.inverse(u, labels_i)\n",
    "            log_probs = model.log_prob(sample, labels_i).sort(0)[1].flip(0)  # sort by log_prob; take argsort idxs; flip high to low\n",
    "            samples.append(sample[log_probs])\n",
    "\n",
    "        samples = torch.cat(samples, dim=0)\n",
    "\n",
    "    # unconditional model\n",
    "    else:\n",
    "        u = model.base_dist.sample((n_row**2, args.n_components)).squeeze()\n",
    "        samples, _ = model.inverse(u)\n",
    "        log_probs = model.log_prob(samples).sort(0)[1].flip(0)  # sort by log_prob; take argsort idxs; flip high to low\n",
    "        samples = samples[log_probs]\n",
    "\n",
    "    # convert and save images\n",
    "    samples = samples.view(samples.shape[0], *args.input_dims)\n",
    "    samples = (torch.sigmoid(samples) - dataset_lam) / (1 - 2 * dataset_lam)\n",
    "    filename = 'generated_samples' + (step != None)*'_epoch_{}'.format(step) + '.png'\n",
    "    save_image(samples, os.path.join(args.output_dir, filename), nrow=n_row, normalize=True)\n",
    "\n",
    "def train_and_evaluate(model, train_loader, test_loader, optimizer, args):\n",
    "    best_eval_logprob = float('-inf')\n",
    "\n",
    "    for i in range(args.start_epoch, args.start_epoch + args.n_epochs):\n",
    "        train(model, train_loader, optimizer, i, args)\n",
    "        eval_logprob, _ = evaluate(model, test_loader, i, args)\n",
    "\n",
    "        # save training checkpoint\n",
    "        torch.save({'epoch': i,\n",
    "                    'model_state': model.state_dict(),\n",
    "                    'optimizer_state': optimizer.state_dict()},\n",
    "                    os.path.join(args.output_dir, 'model_checkpoint.pt'))\n",
    "        # save model only\n",
    "        torch.save(model.state_dict(), os.path.join(args.output_dir, 'model_state.pt'))\n",
    "\n",
    "        # save best state\n",
    "        if eval_logprob > best_eval_logprob:\n",
    "            best_eval_logprob = eval_logprob\n",
    "            torch.save({'epoch': i,\n",
    "                        'model_state': model.state_dict(),\n",
    "                        'optimizer_state': optimizer.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'best_model_checkpoint.pt'))\n",
    "\n",
    "        # plot sample\n",
    "        if args.dataset == 'TOY':\n",
    "            plot_sample_and_density(model, train_loader.dataset.base_dist, args, step=i)\n",
    "        if args.dataset == 'MNIST':\n",
    "            generate(model, train_loader.dataset.lam, args, step=i)\n",
    "\n",
    "# --------------------\n",
    "# Plot\n",
    "# --------------------\n",
    "\n",
    "def plot_density(dist, ax, ranges, flip_var_order=False):\n",
    "    (xmin, xmax), (ymin, ymax) = ranges\n",
    "    # sample uniform grid\n",
    "    n = 200\n",
    "    xx1 = torch.linspace(xmin, xmax, n)\n",
    "    xx2 = torch.linspace(ymin, ymax, n)\n",
    "    xx, yy = torch.meshgrid(xx1, xx2)\n",
    "    xy = torch.stack((xx.flatten(), yy.flatten()), dim=-1).squeeze()\n",
    "\n",
    "    if flip_var_order:\n",
    "        xy = xy.flip(1)\n",
    "\n",
    "    # run uniform grid through model and plot\n",
    "    density = dist.log_prob(xy).exp()\n",
    "    ax.contour(xx, yy, density.view(n,n).data.numpy())\n",
    "\n",
    "    # format\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    ax.set_xticks([xmin, xmax])\n",
    "    ax.set_yticks([ymin, ymax])\n",
    "\n",
    "def plot_dist_sample(data, ax, ranges):\n",
    "    ax.scatter(data[:,0].data.numpy(), data[:,1].data.numpy(), s=10, alpha=0.4)\n",
    "    # format\n",
    "    (xmin, xmax), (ymin, ymax) = ranges\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    ax.set_xticks([xmin, xmax])\n",
    "    ax.set_yticks([ymin, ymax])\n",
    "\n",
    "def plot_sample_and_density(model, target_dist, args, ranges_density=[[-5,20],[-10,10]], ranges_sample=[[-4,4],[-4,4]], step=None):\n",
    "    model.eval()\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(6,3))\n",
    "\n",
    "    # sample target distribution and pass through model\n",
    "    data = target_dist.sample((2000,))\n",
    "    u, _ = model(data)\n",
    "\n",
    "    # plot density and sample\n",
    "    plot_density(model, axs[0], ranges_density, args.flip_var_order)\n",
    "    plot_dist_sample(u, axs[1], ranges_sample)\n",
    "\n",
    "    # format and save\n",
    "    matplotlib.rcParams.update({'xtick.labelsize': 'xx-small', 'ytick.labelsize': 'xx-small'})\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(args.output_dir, 'sample' + (step != None)*'_epoch_{}'.format(step) + '.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Run\n",
    "# --------------------\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # setup file ops\n",
    "    if not os.path.isdir(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "\n",
    "    # setup device\n",
    "    args.device = torch.device('cuda:0' if torch.cuda.is_available() and not args.no_cuda else 'cpu')\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.device.type == 'cuda': torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "    # load data\n",
    "    if args.conditional: assert args.dataset in ['MNIST', 'CIFAR10'], 'Conditional inputs only available for labeled datasets MNIST and CIFAR10.'\n",
    "    train_dataloader, test_dataloader = fetch_dataloaders(args.dataset, args.batch_size, args.device, args.flip_toy_var_order)\n",
    "    args.input_size = train_dataloader.dataset.input_size\n",
    "    args.input_dims = train_dataloader.dataset.input_dims\n",
    "    args.cond_label_size = train_dataloader.dataset.label_size if args.conditional else None\n",
    "\n",
    "    # model\n",
    "    if args.model == 'made':\n",
    "        model = MADE(args.input_size, args.hidden_size, args.n_hidden, args.cond_label_size,\n",
    "                     args.activation_fn, args.input_order)\n",
    "    elif args.model == 'mademog':\n",
    "        assert args.n_components > 1, 'Specify more than 1 component for mixture of gaussians models.'\n",
    "        model = MADEMOG(args.n_components, args.input_size, args.hidden_size, args.n_hidden, args.cond_label_size,\n",
    "                     args.activation_fn, args.input_order)\n",
    "    elif args.model == 'maf':\n",
    "        model = MAF(args.n_blocks, args.input_size, args.hidden_size, args.n_hidden, args.cond_label_size,\n",
    "                    args.activation_fn, args.input_order, batch_norm=not args.no_batch_norm)\n",
    "    elif args.model == 'mafmog':\n",
    "        assert args.n_components > 1, 'Specify more than 1 component for mixture of gaussians models.'\n",
    "        model = MAFMOG(args.n_blocks, args.n_components, args.input_size, args.hidden_size, args.n_hidden, args.cond_label_size,\n",
    "                    args.activation_fn, args.input_order, batch_norm=not args.no_batch_norm)\n",
    "    elif args.model =='realnvp':\n",
    "        model = RealNVP(args.n_blocks, args.input_size, args.hidden_size, args.n_hidden, args.cond_label_size,\n",
    "                        batch_norm=not args.no_batch_norm)\n",
    "    else:\n",
    "        raise ValueError('Unrecognized model.')\n",
    "\n",
    "    model = model.to(args.device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-6)\n",
    "\n",
    "    if args.restore_file:\n",
    "        # load model and optimizer states\n",
    "        state = torch.load(args.restore_file, map_location=args.device)\n",
    "        model.load_state_dict(state['model_state'])\n",
    "        optimizer.load_state_dict(state['optimizer_state'])\n",
    "        args.start_epoch = state['epoch'] + 1\n",
    "        # set up paths\n",
    "        args.output_dir = os.path.dirname(args.restore_file)\n",
    "    args.results_file = os.path.join(args.output_dir, args.results_file)\n",
    "\n",
    "    print('Loaded settings and model:')\n",
    "    print(pprint.pformat(args.__dict__))\n",
    "    print(model)\n",
    "    print(pprint.pformat(args.__dict__), file=open(args.results_file, 'a'))\n",
    "    print(model, file=open(args.results_file, 'a'))\n",
    "\n",
    "    if args.train:\n",
    "        train_and_evaluate(model, train_dataloader, test_dataloader, optimizer, args)\n",
    "\n",
    "    if args.evaluate:\n",
    "        evaluate(model, test_dataloader, None, args)\n",
    "\n",
    "    if args.generate:\n",
    "        if args.dataset == 'TOY':\n",
    "            base_dist = train_dataloader.dataset.base_dist\n",
    "            plot_sample_and_density(model, base_dist, args, ranges_density=[[-15,4],[-3,3]], ranges_sample=[[-1.5,1.5],[-3,3]])\n",
    "        elif args.dataset == 'MNIST':\n",
    "            generate(model, train_dataloader.dataset.lam, args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
